\chapter{Webanalyse} %ca 8-10 Seiten
\label{ch:webanalyse} 

\section{Definitionen und Abgrenzung} % 1-1,5 Seiten
Die Webanalyse (engl. Web Analytics) ist ein Teilbereich der Digitalanalyse (engl. Digital Analytics) und hat als Aufgabe, Webseiten zu analysieren und zu optimieren. Die Abgrenzung zwischen Webanalyse und Digitalanalyse zeigt, dass sich die Webanalyse primär auf Daten bezieht, die aus dem Besuch und der Nutzung von Websites entstehen, wobei die Digitalanalyse den Rahmen erweitert und sich mit der kanalfübergreifenden Ermittlung von Nutzerbezogenen Daten beschäftigt [Hassler, 2019, Kap.1.2]. Die Webanalyse wird von der Digital Analytics Association (DAA) wie folgt definiert: \textit{"Web analytics is the measurement, collection, analysis and reporting of web data for purposes of understanding and optimizing web usage."} [DAA, 2008].

Eine weitere Definition liefert die ISO 19731:2017, welche Webanalyse als Analyse und Berichterstattung über das Verhalten, Aussagen und Stimmungen von Nutzern auf Online-Plattformen beschreibt [International Organization for Standardization (ISO), 2017, Abschnitt 3.93]. Diese Definition hebt zusätzlich die Bedeutung von qualitativen und stimmungsbasierten Daten hervor.

Sie dient unter anderem dazu, die Herkunft von Nutzern (Kanäle), deren Klickverhalten (Klickpfade) sowie die Effektivität von Kampagnen und die Leistung einer Website zu analysieren. Erkenntnisse aus der Webanalyse unterstützen Unternehmen oder Organisationen dabei, ihre Inhalte entsprechend des Nutzerverhaltens zu personalisieren, Schwachstellen in der Customer Journey zu identifizieren und die Nutzererfahrung (User Experience) zu verbessern.

-> Abgrenzung von Webanalyse und Webmonitoring
-> Es gibt viele Bezeichnungen für die Webanalyse.. Digital Analytics, Webanalytics usw... im weiteren Verlauf der Arbeit sollen einfachheitshalber diese Begriffe alle unter der Bezeichnung Webanalyse verschmolzen werden 

\section{Ziele} %1,5 Seiten
Die Webanalyse im Allgemeinem hat das Ziel, auf Grundlage von Daten Erkenntnisse zur Optimierung digitaler Kanäle zu gewinnen. Dabei ist jedoch zu berücksichtigen, dass die Webanalyse keine exakte Wissenschaft ist. Zum Beispiel unterliegen Daten zur Nutzeraktivität, wie Besucher- und Nutzungszahlen, unvermeidbaren Ungenauigkeiten, welche bereits bei der Erhebung und Verarbeitung entstehen können. Fragen nach der genauen Messung aller Besucher, der Unterscheidung von menschlichen und automatisierten Zugriffen sowie der Interpretation von Nutzungsverhalten verdeutlichen die Grenzen dieser Methode. Dennoch liegt der Fokus nicht auf absoluter Präzision, sondern auf der Ableitung verwertbarer Trends und Veränderungen, etwa durch prozentuale Vergleiche. Dieser pragmatische Ansatz ermöglicht es, relevante Erkenntnisse für die Optimierung digitaler Angebote zu gewinnen, ohne dabei auf vollständige Genauigkeit angewiesen zu sein [Hassler, 2019, Kap.1.4].

Der Zweck der Analyse ist es zunächst, einen Überblick über das aktuelle Nutzerverhalten auf dem Bildungsportal zu bekommen. Dies umfasst sowohl die Identifikation der am häufigsten genutzten Inhalte und Funktionen als auch die Ermittlung möglicher Schwachstellen in der Benutzerführung. Dadurch können gezielte Maßnahmen abgeleitet werden, um die Effektivität und Attraktivität des Portals nachhaltig zu steigern. Somit kann die Lösung ebenfalls dafür verwendet werden folgende Ziele zu erreichen: 
\begin{enumerate}
    \item \textbf{Verbesserung der Lernprozesse:}
    Durch die Analyse von Webdaten können Bildungsportale herausfinden, welche Inhalte von Nutzern effektiv genutzt werden und welche weniger relevant sind. Dies ermöglicht eine gezielte Optimierung der Lerninhalte und eine Anpassung an die Bedürfnisse der Lernenden (Piwik PRO, o. J.).
    \item \textbf{Steigerung der Benutzerfreundlichkeit:}
    Die Untersuchung von Nutzerinteraktionen erlaubt es, die Benutzeroberfläche so zu gestalten, dass sie einfacher zu bedienen ist und den Lernprozess unterstützt. Dies trägt zu einer besseren Lernerfahrung bei und erleichtert die Navigation auf der Plattform (Piwik PRO, o. J.).
    \item \textbf{Personalisierung des Lernens:}
    Daten aus der Webanalyse ermöglichen es, individuelle Lernpfade zu entwickeln, die an die Fähigkeiten und Bedürfnisse einzelner Nutzer angepasst sind. Dies sorgt für eine effektivere und nachhaltigere Lernerfahrung (StudySmarter, o. J.).
    \item \textbf{Identifikation von Schwachstellen:} Die Analyse der Interaktionen und Nutzungsdaten kann Schwachstellen in der Customer Journey aufdecken, die anschließend gezielt beseitigt werden können, um die Lernerfahrung weiter zu optimieren (Piwik PRO, o. J.).
\end{enumerate} 

Der erste Schritt zur Erreichung dieser Ziele ist es, eine fundierte Grundlage zu schaffen, um Optimierungspotenziale zu identifizieren und Fortschritte zu messen. Hierbei spielt die Auswahl geeigneter Metriken, Kennzahlen und Key Performance Indicators (KPIs) eine entscheidende Rolle. Sie ermöglichen es, die Interaktionen mit dem Bildungsportal quantifierbar zu machen, um datenbasierte Entscheidungen treffen zu können. Im folgenden Kapitel werden die wichtigsten Metriken, Kennzahlen und KPIs, sowie deren Bedeutung näher erläutert. (Quelle?)

\section{Metriken, Kennzahlen und KPIs} % 1-2
- 
- Definition Metriken, Kennzahlen und KPIs 

\section{Datenerfassung} % 2-2,5 Seiten

\subsection{Logfile-Analyse}
Logdateien (engl. Logfiles) gehören zu den ältesten Methoden, um Nutzungsdaten von Webseiten zu erfassen. Webserver protokollieren seit den Anfängen des Internets sämtliche Anfragen, die an sie gestellt werden, sowie die Antworten, die sie daraufhin liefern [Quelle Suchen]. Diese Protokolle werden in Form von Textdateien gespeichert und bieten eine strukturierte Übersicht über alle Interaktionen, auch „Hits“ genannt, die auf dem Server stattfinden. Die Erstellung von Logfiles erfolgt durch den Webserver, der alle Interaktionen zwischen Nutzer und Webseite aufzeichnet. Sobald ein Nutzer eine Webseite aufruft, sendet sein Browser mehrere Anforderungen (Requests) an den Server, um die verschiedenen Elemente der Seite wie Texte, Bilder oder Skripte zu laden. Der Server liefert daraufhin die angeforderten Inhalte zurück (Responses) und speichert diese Vorgänge zeitgleich in einer Logdatei. Diese Dateien, die in der Regel täglich neu generiert werden, dienen als Quelle für die Analyse von Zugriffen auf die Website. [Hassler, 2019, Kap. 2.2] 

Im Listing~\ref{lst:logfile} ist ein Beispiel eines Logfile-Eintrags im Common Logfile Format (CLF) zu sehen: 

\lstinputlisting[caption=Logfile-Eintrag im CLF {[Fou]}, label={lst:logfile}, language=Apache]{listings/logfile_entry.txt}

Aus diesem Beispiel Eintrag lassen sich folgende Informationen in entsprechender Reihenfolge entnehmen [Apache Software Foundation, o. D.?]: 

\begin{itemize}
    \item IP-Adresse des Aufrufenden
    \item Identitätskennzeichen
    \item Authentifizierter Benutzername
    \item Aufrufdatum und Zeit
    \item URL der aufgerufenen Datei
    \item HTTP-Anfrage
    \item Ergebnis des Aufrufs (Status)
    \item Größe der zurückgegebenen Datei
\end{itemize}

Zusätzlich wird in der Logfile auch die Aufrufmethode (z. B. GET oder POST),  gespeichert, diese Angabe ist jedoch für eine spätere Analyse oft weniger relevant [Hassler, 2019, Kap. 2.2]. Der Vorteil dieser Methode ist es, dass die Datenerfassung unabhängig von Client-Technologien abläuft, da diese serverseitig erfolgt und keine zusätzlichen Technologien wie JavaScript notwendig sind. Zudem werden sämtliche HTTP-Requests, einschließlich der von Bots und Suchmaschinen-Crawlern, erfasst und können leicht in Textform archiviert werden. [Ryte, o.D.]
Der große Nachteil dieses serverseitigen Verfahrens ist es, dass clientseitige Informationen, welche nicht an den Webserver übertragen werden, nicht erfasst werden können. Ebenso werden Interaktionen wie das Anklicken eines Bildes, die Scrolltiefe oder Verweildauer, die das Nutzerverhalten genauer beschreiben, nicht berücksichtigt. Zudem können Seitenaufrufe (engl. Page Views), welche aus dem Browser- Cache geladen werden, nicht in den Logfiles erfasst werden. [COUNTER, 2021]

\subsection{Page Tagging}
Unter Page Tagging versteht man die Einbindung eines kurzen Codeausschnittes (engl. Snippet) in den HTML-Code einer Webseite um Nutzerdaten zu erfassen. Dieses Snippet ist meistens in der Programmiersprache Javascript geschrieben. Beim initialen Aufruf einer Webseite (engl. page request) sendet der Browser eine HTTP-Anfrage an den Webserver, um die für die Darstellung der Seite erforderlichen Inhalte wie Bilder, Videos und Text zu laden. Der Webserver ist dafür verantwortlich, die notwendigen Bestandteile einer Webseite bereitzustellen, damit diese vollständig dargestellt werden kann. Dazu liefert er die angeforderten Dateien über eine HTTP-Antwort an den Webbrowser. Die erste Datei, die der Server sendet, ist eine HTML-Datei, welche die Struktur und den Inhalt der Seite definiert. Diese HTML-Datei enthält unter anderem das zuvor eingebundene Snippet, dass sogenannte Page-Tag, um die Datenerfassung zu ermöglichen. Wenn der Webbrowser nun beim Aufbau der Webseite, dass Page-Tag erkennt, beginnt dieser damit, die im Page-Tag referenzierte Javascript-Bibliothek vom Webserver herunterzuladen. Diese Bibliothek dient dazu, den gleichen Programmcode für mehrere Webseiten nutzbar zu machen. Die heruntergeladene Bibliothek wird schließlich im Cache-Speicher des Webbrowsers hinterlegt, sodass diese bei zukünftigen Seitenaufrufen schneller geladen werden. Nachdem das Page Tag die Bibliothek vom Server geladen hat, wird der Code aus der Bibliothek ausgeführt, um verschiedene Daten aus dem Browser zu sammeln. Dazu gehören unter anderem die Bildschirmauflösung, der Referrer (die Herkunftsseite), der Seiten-Titel und -URL sowie Informationen aus Cookies, welche eventuell bei vorherigen Besuchen gesetzt wurden. Diese gesammelten Daten verbleiben zunächst lokal im Browser [Dykes, 2014, S.70-71]. 

Um die gesammelten Daten an den Webserver zu übertragen, wird ein Bild in der Größe eines Pixels erzeugt, weshalb dieses Verfahren ebenfalls als Pixel-Tracking bezeichnet wird [Hassler, 2019, S.31]. Der Webbrowser erkennt dieses Bild als fehlenden Inhalt an, der geladen werden muss und sendet eine HTTP-Anfrage um es abzurufen. Diese Anfrage wird auch als Image-Request bezeichnet und enthält alle zuvor gesammelten Daten als Parameter in der URL (Uniform Resource Locator). Die Image-Request wird allerdings nicht an den Webserver gesendet, auf welchem die Webseite hinterlegt ist, sondern an einen dedizierten Datensammlungsserver. Wurden die Daten vom Datensammlungsserver empfangen werden die Parameter aus der URL extrahiert und in einer Datenbank gespeichert. Anschließend können die Daten von einer Analyse-Engine verarbeitet werden um zusätzliche Einsichten in das Nutzerverhalten zu gewinnen. Neben der Verarbeitung der Daten kann der Datensammlungsserver in der HTTP-Antwort auch einen Cookie setzen, sofern dieser nicht bereits zuvor schon gesetzt wurde. Dieser Cookie enthält eine eindeutige Kennung (Visitor-ID), die es ermöglicht, dass Gerät des Nutzers bei zukünftigen Besuchen wiederzuerkennen und weitere Informationen mit diesem zu verknüpfen [Dykes, 2014, S.70-71].

\subsection{Events und Interaktionen}

\subsection{DSGVO-Shit}

\section{Tools zur Webanalyse} %3 Seiten